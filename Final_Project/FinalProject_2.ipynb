{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed21826-4412-41bd-b444-f21bb5d36abe",
   "metadata": {},
   "source": [
    "Download from SDSSDR18 all spectra from the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bdf86d1-c7da-4852-b405-5c9a9519363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astroquery.sdss import SDSS\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from astroquery.gaia import Gaia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b258d7dc-a11f-405a-9bea-0e85da828fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE_ID</th>\n",
       "      <th>original_ext_source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3647851003078664832</td>\n",
       "      <td>1237648703516442877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3661958424457651712</td>\n",
       "      <td>1237648704049840359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662377883848390912</td>\n",
       "      <td>1237648704586973290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4419851136648786304</td>\n",
       "      <td>1237648705670152326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4407884120813978752</td>\n",
       "      <td>1237648705678213667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133828</th>\n",
       "      <td>1779912581508627072</td>\n",
       "      <td>1237680273108960003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133829</th>\n",
       "      <td>309194386402373120</td>\n",
       "      <td>1237680285996155390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133830</th>\n",
       "      <td>1883704073988238336</td>\n",
       "      <td>1237680303715582376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133831</th>\n",
       "      <td>2866659811294043904</td>\n",
       "      <td>1237680308016841309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133832</th>\n",
       "      <td>1799404173890542080</td>\n",
       "      <td>1237680327333381385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133833 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  SOURCE_ID  original_ext_source_id\n",
       "0       3647851003078664832     1237648703516442877\n",
       "1       3661958424457651712     1237648704049840359\n",
       "2       3662377883848390912     1237648704586973290\n",
       "3       4419851136648786304     1237648705670152326\n",
       "4       4407884120813978752     1237648705678213667\n",
       "...                     ...                     ...\n",
       "133828  1779912581508627072     1237680273108960003\n",
       "133829   309194386402373120     1237680285996155390\n",
       "133830  1883704073988238336     1237680303715582376\n",
       "133831  2866659811294043904     1237680308016841309\n",
       "133832  1799404173890542080     1237680327333381385\n",
       "\n",
       "[133833 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossmatch = pd.read_csv('/home/thara/Big/gaia_results_all_batches.csv')\n",
    "crossmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fbdc3a8-d1cc-49c9-b652-2383387945e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/2676.\n",
      "Processed batch 100/2676.\n",
      "Processed batch 200/2676.\n",
      "Processed batch 300/2676.\n",
      "Processed batch 400/2676.\n",
      "Processed batch 500/2676.\n",
      "Processed batch 600/2676.\n",
      "Processed batch 700/2676.\n",
      "Processed batch 800/2676.\n",
      "Processed batch 900/2676.\n",
      "Processed batch 1000/2676.\n",
      "Processed batch 1100/2676.\n",
      "Processed batch 1200/2676.\n",
      "Processed batch 1300/2676.\n",
      "Processed batch 1400/2676.\n",
      "Processed batch 1500/2676.\n",
      "Processed batch 1600/2676.\n",
      "Processed batch 1700/2676.\n",
      "Processed batch 1800/2676.\n",
      "Processed batch 1900/2676.\n",
      "Processed batch 2000/2676.\n",
      "Processed batch 2100/2676.\n",
      "Processed batch 2200/2676.\n",
      "Processed batch 2300/2676.\n",
      "Processed batch 2400/2676.\n",
      "Processed batch 2500/2676.\n",
      "Processed batch 2600/2676.\n",
      "Query completed. Results saved to 'specobj_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store results\n",
    "spec_df = pd.DataFrame()\n",
    "\n",
    "# Define a small batch size for querying\n",
    "batch_size = 50  # Small batch size to avoid exceeding query limits\n",
    "max_retries = 3  # Number of retries for failed batches\n",
    "\n",
    "# Process bestObjIDs in small batches\n",
    "best_obj_ids = crossmatch['original_ext_source_id'].unique()\n",
    "for batch_index, i in enumerate(range(0, len(best_obj_ids), batch_size), start=1):\n",
    "    batch = best_obj_ids[i:i + batch_size]\n",
    "    batch_ids = ','.join(map(str, batch))  # Properly format IDs\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT plate, mjd, fiberid, bestObjID\n",
    "    FROM SpecObj\n",
    "    WHERE bestObjID IN ({batch_ids})\n",
    "    \"\"\"\n",
    "    \n",
    "    retries = 0\n",
    "    success = False\n",
    "    \n",
    "    while retries < max_retries and not success:\n",
    "        try:\n",
    "            # Query the SDSS database\n",
    "            result = SDSS.query_sql(query)\n",
    "            if result is not None:\n",
    "                # Append results to the DataFrame\n",
    "                spec_df = pd.concat([spec_df, result.to_pandas()], ignore_index=True)\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if batch_index % 100 == 0 or batch_index == 1:\n",
    "                print(f\"Processed batch {batch_index}/{len(best_obj_ids) // batch_size + 1}.\")\n",
    "            \n",
    "            success = True  # Mark this batch as successful\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error processing batch {i} to {i + len(batch)}: {e}. Retrying ({retries}/{max_retries})...\")\n",
    "            time.sleep(2)  # Add a short delay before retrying\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Batch {i} to {i + len(batch)} failed after {max_retries} retries.\")\n",
    "\n",
    "# Save the results to a file\n",
    "spec_df.to_csv(\"specobj_results.csv\", index=False)\n",
    "print(\"Query completed. Results saved to 'specobj_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73cf9e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "      <th>bestObjID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303</td>\n",
       "      <td>51615</td>\n",
       "      <td>205</td>\n",
       "      <td>1237648703516442877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>299</td>\n",
       "      <td>51671</td>\n",
       "      <td>179</td>\n",
       "      <td>1237648704049840359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>51943</td>\n",
       "      <td>309</td>\n",
       "      <td>1237648704586973290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>311</td>\n",
       "      <td>51665</td>\n",
       "      <td>443</td>\n",
       "      <td>1237648705670152326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>364</td>\n",
       "      <td>52000</td>\n",
       "      <td>380</td>\n",
       "      <td>1237648705678213667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133789</th>\n",
       "      <td>7572</td>\n",
       "      <td>56944</td>\n",
       "      <td>710</td>\n",
       "      <td>1237680273108960003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133790</th>\n",
       "      <td>6265</td>\n",
       "      <td>56248</td>\n",
       "      <td>512</td>\n",
       "      <td>1237680285996155390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133791</th>\n",
       "      <td>6295</td>\n",
       "      <td>56536</td>\n",
       "      <td>434</td>\n",
       "      <td>1237680303715582376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133792</th>\n",
       "      <td>6515</td>\n",
       "      <td>56536</td>\n",
       "      <td>932</td>\n",
       "      <td>1237680308016841309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133793</th>\n",
       "      <td>5960</td>\n",
       "      <td>56097</td>\n",
       "      <td>436</td>\n",
       "      <td>1237680327333381385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133794 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        plate    mjd  fiberid            bestObjID\n",
       "0         303  51615      205  1237648703516442877\n",
       "1         299  51671      179  1237648704049840359\n",
       "2         300  51943      309  1237648704586973290\n",
       "3         311  51665      443  1237648705670152326\n",
       "4         364  52000      380  1237648705678213667\n",
       "...       ...    ...      ...                  ...\n",
       "133789   7572  56944      710  1237680273108960003\n",
       "133790   6265  56248      512  1237680285996155390\n",
       "133791   6295  56536      434  1237680303715582376\n",
       "133792   6515  56536      932  1237680308016841309\n",
       "133793   5960  56097      436  1237680327333381385\n",
       "\n",
       "[133794 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv('/home/thara/Big/specobj_results.csv')\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ec8561-7cb4-4e06-b50a-e4968a791b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 duplicate entries in 'original_ext_source_id'.\n",
      "Duplicate entries:\n",
      "                  SOURCE_ID  original_ext_source_id\n",
      "91       921068247868697728     1237653587407536295\n",
      "92       921068243577055232     1237653587407536295\n",
      "349      706252331821810944     1237660961862189211\n",
      "350      706252331823294464     1237660961862189211\n",
      "406     1541291590181192320     1237661852007727118\n",
      "...                     ...                     ...\n",
      "131800   147929260770079872     1237660558135656804\n",
      "132095   148055807685252352     1237660559209332868\n",
      "132096   148055807686085120     1237660559209332868\n",
      "132667  3706417688227261568     1237661970652397647\n",
      "132668  3706417692523377408     1237661970652397647\n",
      "\n",
      "[77 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in original_ext_source_id\n",
    "duplicate_ids = crossmatch[crossmatch.duplicated(subset='original_ext_source_id', keep=False)]\n",
    "\n",
    "if duplicate_ids.empty:\n",
    "    print(\"All IDs in 'original_ext_source_id' are unique.\")\n",
    "else:\n",
    "    print(f\"Found {len(duplicate_ids)} duplicate entries in 'original_ext_source_id'.\")\n",
    "    print(\"Duplicate entries:\")\n",
    "    print(duplicate_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5f6f4",
   "metadata": {},
   "source": [
    "The `Download.py` script should be used to download the spectra from SDSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4e65cb-6482-44cb-8227-c11eb18fbcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing existing FITS files...\n",
      "Finished processing existing FITS files.\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "fits_dir = \"/home/thara/Big/Spectra\"  # Directory with FITS files\n",
    "parquet_dir = \"/home/thara/Big/Spectra_Parquet\"  # Directory for Parquet files\n",
    "os.makedirs(parquet_dir, exist_ok=True)  # Ensure the Parquet directory exists\n",
    "\n",
    "# Load the specObj list\n",
    "spectra_info = '/home/thara/Big/specobj_results.csv'  # Replace with your file\n",
    "\n",
    "# Define batch size and retry settings\n",
    "batch_size = 1000  # Number of spectra to process in one batch\n",
    "max_retries = 3  # Number of retries for failed downloads\n",
    "\n",
    "# Process existing FITS files\n",
    "print(\"Processing existing FITS files...\")\n",
    "for fits_file in os.listdir(fits_dir):\n",
    "    if fits_file.endswith(\".fits\"):\n",
    "        # Extract plate, mjd, and fiberid from the filename\n",
    "        file_parts = fits_file.split(\"_\")\n",
    "        plate = file_parts[1][5:]\n",
    "        mjd = file_parts[2][3:]\n",
    "        fiberid = file_parts[3][5:].replace(\".fits\", \"\")\n",
    "        \n",
    "        # Define Parquet filename\n",
    "        parquet_file = os.path.join(parquet_dir, f\"spectrum_plate{plate}_mjd{mjd}_fiber{fiberid}.parquet\")\n",
    "        \n",
    "        # Skip if Parquet file already exists\n",
    "        if os.path.exists(parquet_file):\n",
    "            continue\n",
    "        \n",
    "        # Open FITS file and extract data\n",
    "        fits_path = os.path.join(fits_dir, fits_file)\n",
    "        try:\n",
    "            with fits.open(fits_path) as hdul:\n",
    "                data = Table(hdul[1].data)\n",
    "                \n",
    "                # Convert to native byte order\n",
    "                flux = np.array(data[\"flux\"]).byteswap().newbyteorder()\n",
    "                loglam = np.array(data[\"loglam\"]).byteswap().newbyteorder()\n",
    "            \n",
    "            # Save to Parquet\n",
    "            df = pd.DataFrame({\"loglam\": loglam, \"flux\": flux})\n",
    "            df.to_parquet(parquet_file, index=False)\n",
    "            \n",
    "            # Delete the FITS file\n",
    "            os.remove(fits_path)\n",
    "            #print(f\"Processed and deleted: {fits_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fits_file}: {e}\")\n",
    "print(\"Finished processing existing FITS files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ac4fe-c06d-4c6c-8a3e-552de27b2a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flux ratios calculated and saved to 'flux_ratios.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "parquet_dir = \"/home/thara/Big/Spectra_Parquet\"  # Directory for Parquet files\n",
    "\n",
    "# Load the DataFrame with bestObjID, plate, mjd, and fiberid\n",
    "metadata_df = pd.read_csv('/home/thara/Big/specobj_results.csv')  # Replace with your metadata file\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Process each Parquet file\n",
    "for parquet_file in os.listdir(parquet_dir):\n",
    "    if parquet_file.endswith(\".parquet\"):\n",
    "        # Extract plate, mjd, and fiberid from the filename\n",
    "        file_parts = parquet_file.split(\"_\")\n",
    "        plate = int(file_parts[1][5:])\n",
    "        mjd = int(file_parts[2][3:])\n",
    "        fiberid = int(file_parts[3][5:].replace(\".parquet\", \"\"))\n",
    "        \n",
    "        # Load the Parquet file\n",
    "        parquet_path = os.path.join(parquet_dir, parquet_file)\n",
    "        try:\n",
    "            data = pd.read_parquet(parquet_path)\n",
    "            \n",
    "            # Convert loglam to lam\n",
    "            data['lam'] = 10 ** data['loglam']\n",
    "            \n",
    "            # Define wavelength ranges\n",
    "            range1 = (3850, 6800)  # First range\n",
    "            range2 = (6400, 9150)  # Second range\n",
    "            \n",
    "            # Calculate integrated flux for each range\n",
    "            mask1 = (data['lam'] >= range1[0]) & (data['lam'] <= range1[1])\n",
    "            mask2 = (data['lam'] >= range2[0]) & (data['lam'] <= range2[1])\n",
    "            \n",
    "            flux1 = np.trapz(data.loc[mask1, 'flux'], data.loc[mask1, 'lam'])\n",
    "            flux2 = np.trapz(data.loc[mask2, 'flux'], data.loc[mask2, 'lam'])\n",
    "            \n",
    "            # Calculate the flux ratio\n",
    "            flux_ratio = flux1 / flux2 if flux2 != 0 else np.nan\n",
    "            \n",
    "            # Find the bestObjID from metadata\n",
    "            matched_row = metadata_df[\n",
    "                (metadata_df['plate'] == plate) & \n",
    "                (metadata_df['mjd'] == mjd) & \n",
    "                (metadata_df['fiberid'] == fiberid)\n",
    "            ]\n",
    "            \n",
    "            if not matched_row.empty:\n",
    "                bestObjID = matched_row['bestObjID'].values[0]\n",
    "                results.append({'bestObjID': bestObjID, 'plate': plate, 'mjd': mjd, 'fiberid': fiberid, 'flux_ratio': flux_ratio})\n",
    "            else:\n",
    "                print(f\"No match found for Parquet file: {parquet_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {parquet_file}: {e}\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\"/home/thara/Big/flux_ratios.csv\", index=False)\n",
    "\n",
    "print(\"Flux ratios calculated and saved to 'flux_ratios.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "592dda54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bestObjID</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "      <th>flux_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1237663784201486401</td>\n",
       "      <td>7868</td>\n",
       "      <td>57006</td>\n",
       "      <td>816</td>\n",
       "      <td>2.177181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1237665548902859021</td>\n",
       "      <td>2202</td>\n",
       "      <td>53566</td>\n",
       "      <td>290</td>\n",
       "      <td>2.016669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237654380901957961</td>\n",
       "      <td>7512</td>\n",
       "      <td>56777</td>\n",
       "      <td>134</td>\n",
       "      <td>1.728434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1237655693011845153</td>\n",
       "      <td>918</td>\n",
       "      <td>52404</td>\n",
       "      <td>515</td>\n",
       "      <td>1.013675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1237667322185580645</td>\n",
       "      <td>2236</td>\n",
       "      <td>53729</td>\n",
       "      <td>1</td>\n",
       "      <td>0.747908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133789</th>\n",
       "      <td>1237680273108960003</td>\n",
       "      <td>7572</td>\n",
       "      <td>56944</td>\n",
       "      <td>710</td>\n",
       "      <td>1.510391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133790</th>\n",
       "      <td>1237680285996155390</td>\n",
       "      <td>6265</td>\n",
       "      <td>56248</td>\n",
       "      <td>512</td>\n",
       "      <td>1.797892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133791</th>\n",
       "      <td>1237680303715582376</td>\n",
       "      <td>6295</td>\n",
       "      <td>56536</td>\n",
       "      <td>434</td>\n",
       "      <td>0.798917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133792</th>\n",
       "      <td>1237680308016841309</td>\n",
       "      <td>6515</td>\n",
       "      <td>56536</td>\n",
       "      <td>932</td>\n",
       "      <td>1.233320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133793</th>\n",
       "      <td>1237680327333381385</td>\n",
       "      <td>5960</td>\n",
       "      <td>56097</td>\n",
       "      <td>436</td>\n",
       "      <td>1.524748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133794 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bestObjID  plate    mjd  fiberid  flux_ratio\n",
       "0       1237663784201486401   7868  57006      816    2.177181\n",
       "1       1237665548902859021   2202  53566      290    2.016669\n",
       "2       1237654380901957961   7512  56777      134    1.728434\n",
       "3       1237655693011845153    918  52404      515    1.013675\n",
       "4       1237667322185580645   2236  53729        1    0.747908\n",
       "...                     ...    ...    ...      ...         ...\n",
       "133789  1237680273108960003   7572  56944      710    1.510391\n",
       "133790  1237680285996155390   6265  56248      512    1.797892\n",
       "133791  1237680303715582376   6295  56536      434    0.798917\n",
       "133792  1237680308016841309   6515  56536      932    1.233320\n",
       "133793  1237680327333381385   5960  56097      436    1.524748\n",
       "\n",
       "[133794 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8647d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 133794\n",
      "Filtered number of rows: 133756\n"
     ]
    }
   ],
   "source": [
    "# Assuming `duplicate_ids` contains the duplicate 'original_ext_source_id'\n",
    "duplicate_ids_set = set(duplicate_ids['original_ext_source_id'])\n",
    "\n",
    "# Remove rows from results_df where 'bestObjID' is in duplicate_ids\n",
    "filtered_results_df = results_df[~results_df['bestObjID'].isin(duplicate_ids_set)]\n",
    "\n",
    "# Check the number of rows before and after filtering\n",
    "print(f\"Original number of rows: {len(results_df)}\")\n",
    "print(f\"Filtered number of rows: {len(filtered_results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5c68a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bestObjID</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "      <th>flux_ratio</th>\n",
       "      <th>SOURCE_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1237663784201486401</td>\n",
       "      <td>7868</td>\n",
       "      <td>57006</td>\n",
       "      <td>816</td>\n",
       "      <td>2.177181</td>\n",
       "      <td>2543193557806316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1237665548902859021</td>\n",
       "      <td>2202</td>\n",
       "      <td>53566</td>\n",
       "      <td>290</td>\n",
       "      <td>2.016669</td>\n",
       "      <td>4463606678617356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237654380901957961</td>\n",
       "      <td>7512</td>\n",
       "      <td>56777</td>\n",
       "      <td>134</td>\n",
       "      <td>1.728434</td>\n",
       "      <td>1014204471947028992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1237655693011845153</td>\n",
       "      <td>918</td>\n",
       "      <td>52404</td>\n",
       "      <td>515</td>\n",
       "      <td>1.013675</td>\n",
       "      <td>3652568217199181440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1237667322185580645</td>\n",
       "      <td>2236</td>\n",
       "      <td>53729</td>\n",
       "      <td>1</td>\n",
       "      <td>0.747908</td>\n",
       "      <td>3961435288437544320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133751</th>\n",
       "      <td>1237680273108960003</td>\n",
       "      <td>7572</td>\n",
       "      <td>56944</td>\n",
       "      <td>710</td>\n",
       "      <td>1.510391</td>\n",
       "      <td>1779912581508627072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133752</th>\n",
       "      <td>1237680285996155390</td>\n",
       "      <td>6265</td>\n",
       "      <td>56248</td>\n",
       "      <td>512</td>\n",
       "      <td>1.797892</td>\n",
       "      <td>309194386402373120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133753</th>\n",
       "      <td>1237680303715582376</td>\n",
       "      <td>6295</td>\n",
       "      <td>56536</td>\n",
       "      <td>434</td>\n",
       "      <td>0.798917</td>\n",
       "      <td>1883704073988238336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133754</th>\n",
       "      <td>1237680308016841309</td>\n",
       "      <td>6515</td>\n",
       "      <td>56536</td>\n",
       "      <td>932</td>\n",
       "      <td>1.233320</td>\n",
       "      <td>2866659811294043904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133755</th>\n",
       "      <td>1237680327333381385</td>\n",
       "      <td>5960</td>\n",
       "      <td>56097</td>\n",
       "      <td>436</td>\n",
       "      <td>1.524748</td>\n",
       "      <td>1799404173890542080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133756 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bestObjID  plate    mjd  fiberid  flux_ratio  \\\n",
       "0       1237663784201486401   7868  57006      816    2.177181   \n",
       "1       1237665548902859021   2202  53566      290    2.016669   \n",
       "2       1237654380901957961   7512  56777      134    1.728434   \n",
       "3       1237655693011845153    918  52404      515    1.013675   \n",
       "4       1237667322185580645   2236  53729        1    0.747908   \n",
       "...                     ...    ...    ...      ...         ...   \n",
       "133751  1237680273108960003   7572  56944      710    1.510391   \n",
       "133752  1237680285996155390   6265  56248      512    1.797892   \n",
       "133753  1237680303715582376   6295  56536      434    0.798917   \n",
       "133754  1237680308016841309   6515  56536      932    1.233320   \n",
       "133755  1237680327333381385   5960  56097      436    1.524748   \n",
       "\n",
       "                  SOURCE_ID  \n",
       "0       2543193557806316800  \n",
       "1       4463606678617356800  \n",
       "2       1014204471947028992  \n",
       "3       3652568217199181440  \n",
       "4       3961435288437544320  \n",
       "...                     ...  \n",
       "133751  1779912581508627072  \n",
       "133752   309194386402373120  \n",
       "133753  1883704073988238336  \n",
       "133754  2866659811294043904  \n",
       "133755  1799404173890542080  \n",
       "\n",
       "[133756 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'bestObjID' (filtered_results_df) and 'original_ext_source_id' (crossmatch)\n",
    "merged_results_df = filtered_results_df.merge(\n",
    "    crossmatch[['SOURCE_ID', 'original_ext_source_id']],\n",
    "    left_on='bestObjID', \n",
    "    right_on='original_ext_source_id',\n",
    "    how='left'  # Use 'left' to retain all rows in filtered_results_df\n",
    ")\n",
    "\n",
    "# Drop the 'original_ext_source_id' column if it's no longer needed\n",
    "merged_results_df = merged_results_df.drop(columns=['original_ext_source_id'])\n",
    "merged_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca7126",
   "metadata": {},
   "source": [
    "Now we will proceed to download the GAIA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9c8dee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 (0 to 1000 IDs)... Attempt 1\n",
      "Batch 1 returned 1000 rows.\n",
      "Batch 1 completed. Total rows saved: 1000\n",
      "Processing batch 2 (1000 to 2000 IDs)... Attempt 1\n",
      "Batch 2 returned 1000 rows.\n",
      "Batch 2 completed. Total rows saved: 2000\n",
      "Processing batch 3 (2000 to 3000 IDs)... Attempt 1\n",
      "Batch 3 returned 1000 rows.\n",
      "Batch 3 completed. Total rows saved: 3000\n",
      "Processing batch 4 (3000 to 4000 IDs)... Attempt 1\n",
      "Batch 4 returned 1000 rows.\n",
      "Batch 4 completed. Total rows saved: 4000\n",
      "Processing batch 5 (4000 to 5000 IDs)... Attempt 1\n",
      "Batch 5 returned 1000 rows.\n",
      "Batch 5 completed. Total rows saved: 5000\n",
      "Processing batch 6 (5000 to 6000 IDs)... Attempt 1\n",
      "Batch 6 returned 1000 rows.\n",
      "Batch 6 completed. Total rows saved: 6000\n",
      "Processing batch 7 (6000 to 7000 IDs)... Attempt 1\n",
      "Batch 7 returned 1000 rows.\n",
      "Batch 7 completed. Total rows saved: 7000\n",
      "Processing batch 8 (7000 to 8000 IDs)... Attempt 1\n",
      "Batch 8 returned 1000 rows.\n",
      "Batch 8 completed. Total rows saved: 8000\n",
      "Processing batch 9 (8000 to 9000 IDs)... Attempt 1\n",
      "Batch 9 returned 1000 rows.\n",
      "Batch 9 completed. Total rows saved: 9000\n",
      "Processing batch 10 (9000 to 10000 IDs)... Attempt 1\n",
      "Batch 10 returned 1000 rows.\n",
      "Batch 10 completed. Total rows saved: 10000\n",
      "Processing batch 11 (10000 to 11000 IDs)... Attempt 1\n",
      "Batch 11 returned 1000 rows.\n",
      "Batch 11 completed. Total rows saved: 11000\n",
      "Processing batch 12 (11000 to 12000 IDs)... Attempt 1\n",
      "Batch 12 returned 1000 rows.\n",
      "Batch 12 completed. Total rows saved: 12000\n",
      "Processing batch 13 (12000 to 13000 IDs)... Attempt 1\n",
      "Batch 13 returned 1000 rows.\n",
      "Batch 13 completed. Total rows saved: 13000\n",
      "Processing batch 14 (13000 to 14000 IDs)... Attempt 1\n",
      "Batch 14 returned 1000 rows.\n",
      "Batch 14 completed. Total rows saved: 14000\n",
      "Processing batch 15 (14000 to 15000 IDs)... Attempt 1\n",
      "Batch 15 returned 1000 rows.\n",
      "Batch 15 completed. Total rows saved: 15000\n",
      "Processing batch 16 (15000 to 16000 IDs)... Attempt 1\n",
      "Batch 16 returned 1000 rows.\n",
      "Batch 16 completed. Total rows saved: 16000\n",
      "Processing batch 17 (16000 to 17000 IDs)... Attempt 1\n",
      "Batch 17 returned 1000 rows.\n",
      "Batch 17 completed. Total rows saved: 17000\n",
      "Processing batch 18 (17000 to 18000 IDs)... Attempt 1\n",
      "Batch 18 returned 1000 rows.\n",
      "Batch 18 completed. Total rows saved: 18000\n",
      "Processing batch 19 (18000 to 19000 IDs)... Attempt 1\n",
      "Batch 19 returned 1000 rows.\n",
      "Batch 19 completed. Total rows saved: 19000\n",
      "Processing batch 20 (19000 to 20000 IDs)... Attempt 1\n",
      "Batch 20 returned 1000 rows.\n",
      "Batch 20 completed. Total rows saved: 20000\n",
      "Processing batch 21 (20000 to 21000 IDs)... Attempt 1\n",
      "Batch 21 returned 1000 rows.\n",
      "Batch 21 completed. Total rows saved: 21000\n",
      "Processing batch 22 (21000 to 22000 IDs)... Attempt 1\n",
      "Batch 22 returned 1000 rows.\n",
      "Batch 22 completed. Total rows saved: 22000\n",
      "Processing batch 23 (22000 to 23000 IDs)... Attempt 1\n",
      "Batch 23 returned 1000 rows.\n",
      "Batch 23 completed. Total rows saved: 23000\n",
      "Processing batch 24 (23000 to 24000 IDs)... Attempt 1\n",
      "Batch 24 returned 1000 rows.\n",
      "Batch 24 completed. Total rows saved: 24000\n",
      "Processing batch 25 (24000 to 25000 IDs)... Attempt 1\n",
      "Batch 25 returned 1000 rows.\n",
      "Batch 25 completed. Total rows saved: 25000\n",
      "Processing batch 26 (25000 to 26000 IDs)... Attempt 1\n",
      "Batch 26 returned 1000 rows.\n",
      "Batch 26 completed. Total rows saved: 26000\n",
      "Processing batch 27 (26000 to 27000 IDs)... Attempt 1\n",
      "Batch 27 returned 1000 rows.\n",
      "Batch 27 completed. Total rows saved: 27000\n",
      "Processing batch 28 (27000 to 28000 IDs)... Attempt 1\n",
      "Batch 28 returned 1000 rows.\n",
      "Batch 28 completed. Total rows saved: 28000\n",
      "Processing batch 29 (28000 to 29000 IDs)... Attempt 1\n",
      "Batch 29 returned 1000 rows.\n",
      "Batch 29 completed. Total rows saved: 29000\n",
      "Processing batch 30 (29000 to 30000 IDs)... Attempt 1\n",
      "Batch 30 returned 1000 rows.\n",
      "Batch 30 completed. Total rows saved: 30000\n",
      "Processing batch 31 (30000 to 31000 IDs)... Attempt 1\n",
      "Batch 31 returned 1000 rows.\n",
      "Batch 31 completed. Total rows saved: 31000\n",
      "Processing batch 32 (31000 to 32000 IDs)... Attempt 1\n",
      "Batch 32 returned 1000 rows.\n",
      "Batch 32 completed. Total rows saved: 32000\n",
      "Processing batch 33 (32000 to 33000 IDs)... Attempt 1\n",
      "Batch 33 returned 1000 rows.\n",
      "Batch 33 completed. Total rows saved: 33000\n",
      "Processing batch 34 (33000 to 34000 IDs)... Attempt 1\n",
      "Batch 34 returned 1000 rows.\n",
      "Batch 34 completed. Total rows saved: 34000\n",
      "Processing batch 35 (34000 to 35000 IDs)... Attempt 1\n",
      "Batch 35 returned 1000 rows.\n",
      "Batch 35 completed. Total rows saved: 35000\n",
      "Processing batch 36 (35000 to 36000 IDs)... Attempt 1\n",
      "Batch 36 returned 1000 rows.\n",
      "Batch 36 completed. Total rows saved: 36000\n",
      "Processing batch 37 (36000 to 37000 IDs)... Attempt 1\n",
      "Batch 37 returned 1000 rows.\n",
      "Batch 37 completed. Total rows saved: 37000\n",
      "Processing batch 38 (37000 to 38000 IDs)... Attempt 1\n",
      "Batch 38 returned 1000 rows.\n",
      "Batch 38 completed. Total rows saved: 38000\n",
      "Processing batch 39 (38000 to 39000 IDs)... Attempt 1\n",
      "Batch 39 returned 1000 rows.\n",
      "Batch 39 completed. Total rows saved: 39000\n",
      "Processing batch 40 (39000 to 40000 IDs)... Attempt 1\n",
      "Batch 40 returned 1000 rows.\n",
      "Batch 40 completed. Total rows saved: 40000\n",
      "Processing batch 41 (40000 to 41000 IDs)... Attempt 1\n",
      "Batch 41 returned 1000 rows.\n",
      "Batch 41 completed. Total rows saved: 41000\n",
      "Processing batch 42 (41000 to 42000 IDs)... Attempt 1\n",
      "Batch 42 returned 1000 rows.\n",
      "Batch 42 completed. Total rows saved: 42000\n",
      "Processing batch 43 (42000 to 43000 IDs)... Attempt 1\n",
      "Batch 43 returned 1000 rows.\n",
      "Batch 43 completed. Total rows saved: 43000\n",
      "Processing batch 44 (43000 to 44000 IDs)... Attempt 1\n",
      "Batch 44 returned 1000 rows.\n",
      "Batch 44 completed. Total rows saved: 44000\n",
      "Processing batch 45 (44000 to 45000 IDs)... Attempt 1\n",
      "Batch 45 returned 1000 rows.\n",
      "Batch 45 completed. Total rows saved: 45000\n",
      "Processing batch 46 (45000 to 46000 IDs)... Attempt 1\n",
      "Batch 46 returned 1000 rows.\n",
      "Batch 46 completed. Total rows saved: 46000\n",
      "Processing batch 47 (46000 to 47000 IDs)... Attempt 1\n",
      "Batch 47 returned 1000 rows.\n",
      "Batch 47 completed. Total rows saved: 47000\n",
      "Processing batch 48 (47000 to 48000 IDs)... Attempt 1\n",
      "Batch 48 returned 1000 rows.\n",
      "Batch 48 completed. Total rows saved: 48000\n",
      "Processing batch 49 (48000 to 49000 IDs)... Attempt 1\n",
      "Batch 49 returned 1000 rows.\n",
      "Batch 49 completed. Total rows saved: 49000\n",
      "Processing batch 50 (49000 to 50000 IDs)... Attempt 1\n",
      "Batch 50 returned 1000 rows.\n",
      "Batch 50 completed. Total rows saved: 50000\n",
      "Processing batch 51 (50000 to 51000 IDs)... Attempt 1\n",
      "Batch 51 returned 1000 rows.\n",
      "Batch 51 completed. Total rows saved: 51000\n",
      "Processing batch 52 (51000 to 52000 IDs)... Attempt 1\n",
      "Batch 52 returned 1000 rows.\n",
      "Batch 52 completed. Total rows saved: 52000\n",
      "Processing batch 53 (52000 to 53000 IDs)... Attempt 1\n",
      "Batch 53 returned 1000 rows.\n",
      "Batch 53 completed. Total rows saved: 53000\n",
      "Processing batch 54 (53000 to 54000 IDs)... Attempt 1\n",
      "Batch 54 returned 1000 rows.\n",
      "Batch 54 completed. Total rows saved: 54000\n",
      "Processing batch 55 (54000 to 55000 IDs)... Attempt 1\n",
      "Batch 55 returned 1000 rows.\n",
      "Batch 55 completed. Total rows saved: 55000\n",
      "Processing batch 56 (55000 to 56000 IDs)... Attempt 1\n",
      "Batch 56 returned 1000 rows.\n",
      "Batch 56 completed. Total rows saved: 56000\n",
      "Processing batch 57 (56000 to 57000 IDs)... Attempt 1\n",
      "Batch 57 returned 1000 rows.\n",
      "Batch 57 completed. Total rows saved: 57000\n",
      "Processing batch 58 (57000 to 58000 IDs)... Attempt 1\n",
      "Batch 58 returned 1000 rows.\n",
      "Batch 58 completed. Total rows saved: 58000\n",
      "Processing batch 59 (58000 to 59000 IDs)... Attempt 1\n",
      "Batch 59 returned 1000 rows.\n",
      "Batch 59 completed. Total rows saved: 59000\n",
      "Processing batch 60 (59000 to 60000 IDs)... Attempt 1\n",
      "Batch 60 returned 1000 rows.\n",
      "Batch 60 completed. Total rows saved: 60000\n",
      "Processing batch 61 (60000 to 61000 IDs)... Attempt 1\n",
      "Batch 61 returned 1000 rows.\n",
      "Batch 61 completed. Total rows saved: 61000\n",
      "Processing batch 62 (61000 to 62000 IDs)... Attempt 1\n",
      "Batch 62 returned 1000 rows.\n",
      "Batch 62 completed. Total rows saved: 62000\n",
      "Processing batch 63 (62000 to 63000 IDs)... Attempt 1\n",
      "Batch 63 returned 1000 rows.\n",
      "Batch 63 completed. Total rows saved: 63000\n",
      "Processing batch 64 (63000 to 64000 IDs)... Attempt 1\n",
      "Batch 64 returned 1000 rows.\n",
      "Batch 64 completed. Total rows saved: 64000\n",
      "Processing batch 65 (64000 to 65000 IDs)... Attempt 1\n",
      "Batch 65 returned 1000 rows.\n",
      "Batch 65 completed. Total rows saved: 65000\n",
      "Processing batch 66 (65000 to 66000 IDs)... Attempt 1\n",
      "Batch 66 returned 1000 rows.\n",
      "Batch 66 completed. Total rows saved: 66000\n",
      "Processing batch 67 (66000 to 67000 IDs)... Attempt 1\n",
      "Batch 67 returned 1000 rows.\n",
      "Batch 67 completed. Total rows saved: 67000\n",
      "Processing batch 68 (67000 to 68000 IDs)... Attempt 1\n",
      "Batch 68 returned 1000 rows.\n",
      "Batch 68 completed. Total rows saved: 68000\n",
      "Processing batch 69 (68000 to 69000 IDs)... Attempt 1\n",
      "Batch 69 returned 1000 rows.\n",
      "Batch 69 completed. Total rows saved: 69000\n",
      "Processing batch 70 (69000 to 70000 IDs)... Attempt 1\n",
      "Batch 70 returned 1000 rows.\n",
      "Batch 70 completed. Total rows saved: 70000\n",
      "Processing batch 71 (70000 to 71000 IDs)... Attempt 1\n",
      "Batch 71 returned 1000 rows.\n",
      "Batch 71 completed. Total rows saved: 71000\n",
      "Processing batch 72 (71000 to 72000 IDs)... Attempt 1\n",
      "Batch 72 returned 1000 rows.\n",
      "Batch 72 completed. Total rows saved: 72000\n",
      "Processing batch 73 (72000 to 73000 IDs)... Attempt 1\n",
      "Batch 73 returned 1000 rows.\n",
      "Batch 73 completed. Total rows saved: 73000\n",
      "Processing batch 74 (73000 to 74000 IDs)... Attempt 1\n",
      "Batch 74 returned 1000 rows.\n",
      "Batch 74 completed. Total rows saved: 74000\n",
      "Processing batch 75 (74000 to 75000 IDs)... Attempt 1\n",
      "Batch 75 returned 1000 rows.\n",
      "Batch 75 completed. Total rows saved: 75000\n",
      "Processing batch 76 (75000 to 76000 IDs)... Attempt 1\n",
      "Batch 76 returned 1000 rows.\n",
      "Batch 76 completed. Total rows saved: 76000\n",
      "Processing batch 77 (76000 to 77000 IDs)... Attempt 1\n",
      "Batch 77 returned 1000 rows.\n",
      "Batch 77 completed. Total rows saved: 77000\n",
      "Processing batch 78 (77000 to 78000 IDs)... Attempt 1\n",
      "Batch 78 returned 1000 rows.\n",
      "Batch 78 completed. Total rows saved: 78000\n",
      "Processing batch 79 (78000 to 79000 IDs)... Attempt 1\n",
      "Batch 79 returned 1000 rows.\n",
      "Batch 79 completed. Total rows saved: 79000\n",
      "Processing batch 80 (79000 to 80000 IDs)... Attempt 1\n",
      "Batch 80 returned 1000 rows.\n",
      "Batch 80 completed. Total rows saved: 80000\n",
      "Processing batch 81 (80000 to 81000 IDs)... Attempt 1\n",
      "Batch 81 returned 1000 rows.\n",
      "Batch 81 completed. Total rows saved: 81000\n",
      "Processing batch 82 (81000 to 82000 IDs)... Attempt 1\n",
      "Batch 82 returned 1000 rows.\n",
      "Batch 82 completed. Total rows saved: 82000\n",
      "Processing batch 83 (82000 to 83000 IDs)... Attempt 1\n",
      "Batch 83 returned 1000 rows.\n",
      "Batch 83 completed. Total rows saved: 83000\n",
      "Processing batch 84 (83000 to 84000 IDs)... Attempt 1\n",
      "Batch 84 returned 1000 rows.\n",
      "Batch 84 completed. Total rows saved: 84000\n",
      "Processing batch 85 (84000 to 85000 IDs)... Attempt 1\n",
      "Batch 85 returned 1000 rows.\n",
      "Batch 85 completed. Total rows saved: 85000\n",
      "Processing batch 86 (85000 to 86000 IDs)... Attempt 1\n",
      "Batch 86 returned 1000 rows.\n",
      "Batch 86 completed. Total rows saved: 86000\n",
      "Processing batch 87 (86000 to 87000 IDs)... Attempt 1\n",
      "Batch 87 returned 1000 rows.\n",
      "Batch 87 completed. Total rows saved: 87000\n",
      "Processing batch 88 (87000 to 88000 IDs)... Attempt 1\n",
      "Batch 88 returned 1000 rows.\n",
      "Batch 88 completed. Total rows saved: 88000\n",
      "Processing batch 89 (88000 to 89000 IDs)... Attempt 1\n",
      "Batch 89 returned 1000 rows.\n",
      "Batch 89 completed. Total rows saved: 89000\n",
      "Processing batch 90 (89000 to 90000 IDs)... Attempt 1\n",
      "Batch 90 returned 1000 rows.\n",
      "Batch 90 completed. Total rows saved: 90000\n",
      "Processing batch 91 (90000 to 91000 IDs)... Attempt 1\n",
      "Batch 91 returned 1000 rows.\n",
      "Batch 91 completed. Total rows saved: 91000\n",
      "Processing batch 92 (91000 to 92000 IDs)... Attempt 1\n",
      "Batch 92 returned 1000 rows.\n",
      "Batch 92 completed. Total rows saved: 92000\n",
      "Processing batch 93 (92000 to 93000 IDs)... Attempt 1\n",
      "Batch 93 returned 1000 rows.\n",
      "Batch 93 completed. Total rows saved: 93000\n",
      "Processing batch 94 (93000 to 94000 IDs)... Attempt 1\n",
      "Batch 94 returned 1000 rows.\n",
      "Batch 94 completed. Total rows saved: 94000\n",
      "Processing batch 95 (94000 to 95000 IDs)... Attempt 1\n",
      "Batch 95 returned 1000 rows.\n",
      "Batch 95 completed. Total rows saved: 95000\n",
      "Processing batch 96 (95000 to 96000 IDs)... Attempt 1\n",
      "Batch 96 returned 1000 rows.\n",
      "Batch 96 completed. Total rows saved: 96000\n",
      "Processing batch 97 (96000 to 97000 IDs)... Attempt 1\n",
      "Batch 97 returned 1000 rows.\n",
      "Batch 97 completed. Total rows saved: 97000\n",
      "Processing batch 98 (97000 to 98000 IDs)... Attempt 1\n",
      "Batch 98 returned 1000 rows.\n",
      "Batch 98 completed. Total rows saved: 98000\n",
      "Processing batch 99 (98000 to 99000 IDs)... Attempt 1\n",
      "Batch 99 returned 1000 rows.\n",
      "Batch 99 completed. Total rows saved: 99000\n",
      "Processing batch 100 (99000 to 100000 IDs)... Attempt 1\n",
      "Batch 100 returned 1000 rows.\n",
      "Batch 100 completed. Total rows saved: 100000\n",
      "Processing batch 101 (100000 to 101000 IDs)... Attempt 1\n",
      "Batch 101 returned 1000 rows.\n",
      "Batch 101 completed. Total rows saved: 101000\n",
      "Processing batch 102 (101000 to 102000 IDs)... Attempt 1\n",
      "Batch 102 returned 1000 rows.\n",
      "Batch 102 completed. Total rows saved: 102000\n",
      "Processing batch 103 (102000 to 103000 IDs)... Attempt 1\n",
      "Batch 103 returned 1000 rows.\n",
      "Batch 103 completed. Total rows saved: 103000\n",
      "Processing batch 104 (103000 to 104000 IDs)... Attempt 1\n",
      "Batch 104 returned 1000 rows.\n",
      "Batch 104 completed. Total rows saved: 104000\n",
      "Processing batch 105 (104000 to 105000 IDs)... Attempt 1\n",
      "Batch 105 returned 1000 rows.\n",
      "Batch 105 completed. Total rows saved: 105000\n",
      "Processing batch 106 (105000 to 106000 IDs)... Attempt 1\n",
      "Batch 106 returned 1000 rows.\n",
      "Batch 106 completed. Total rows saved: 106000\n",
      "Processing batch 107 (106000 to 107000 IDs)... Attempt 1\n",
      "Batch 107 returned 1000 rows.\n",
      "Batch 107 completed. Total rows saved: 107000\n",
      "Processing batch 108 (107000 to 108000 IDs)... Attempt 1\n",
      "Batch 108 returned 1000 rows.\n",
      "Batch 108 completed. Total rows saved: 108000\n",
      "Processing batch 109 (108000 to 109000 IDs)... Attempt 1\n",
      "Batch 109 returned 1000 rows.\n",
      "Batch 109 completed. Total rows saved: 109000\n",
      "Processing batch 110 (109000 to 110000 IDs)... Attempt 1\n",
      "Batch 110 returned 1000 rows.\n",
      "Batch 110 completed. Total rows saved: 110000\n",
      "Processing batch 111 (110000 to 111000 IDs)... Attempt 1\n",
      "Batch 111 returned 1000 rows.\n",
      "Batch 111 completed. Total rows saved: 111000\n",
      "Processing batch 112 (111000 to 112000 IDs)... Attempt 1\n",
      "Batch 112 returned 1000 rows.\n",
      "Batch 112 completed. Total rows saved: 112000\n",
      "Processing batch 113 (112000 to 113000 IDs)... Attempt 1\n",
      "Batch 113 returned 1000 rows.\n",
      "Batch 113 completed. Total rows saved: 113000\n",
      "Processing batch 114 (113000 to 114000 IDs)... Attempt 1\n",
      "Batch 114 returned 1000 rows.\n",
      "Batch 114 completed. Total rows saved: 114000\n",
      "Processing batch 115 (114000 to 115000 IDs)... Attempt 1\n",
      "Batch 115 returned 1000 rows.\n",
      "Batch 115 completed. Total rows saved: 115000\n",
      "Processing batch 116 (115000 to 116000 IDs)... Attempt 1\n",
      "Batch 116 returned 1000 rows.\n",
      "Batch 116 completed. Total rows saved: 116000\n",
      "Processing batch 117 (116000 to 117000 IDs)... Attempt 1\n",
      "Batch 117 returned 1000 rows.\n",
      "Batch 117 completed. Total rows saved: 117000\n",
      "Processing batch 118 (117000 to 118000 IDs)... Attempt 1\n",
      "Batch 118 returned 1000 rows.\n",
      "Batch 118 completed. Total rows saved: 118000\n",
      "Processing batch 119 (118000 to 119000 IDs)... Attempt 1\n",
      "Batch 119 returned 1000 rows.\n",
      "Batch 119 completed. Total rows saved: 119000\n",
      "Processing batch 120 (119000 to 120000 IDs)... Attempt 1\n",
      "Batch 120 returned 1000 rows.\n",
      "Batch 120 completed. Total rows saved: 120000\n",
      "Processing batch 121 (120000 to 121000 IDs)... Attempt 1\n",
      "Batch 121 returned 1000 rows.\n",
      "Batch 121 completed. Total rows saved: 121000\n",
      "Processing batch 122 (121000 to 122000 IDs)... Attempt 1\n",
      "Batch 122 returned 1000 rows.\n",
      "Batch 122 completed. Total rows saved: 122000\n",
      "Processing batch 123 (122000 to 123000 IDs)... Attempt 1\n",
      "Batch 123 returned 1000 rows.\n",
      "Batch 123 completed. Total rows saved: 123000\n",
      "Processing batch 124 (123000 to 124000 IDs)... Attempt 1\n",
      "Batch 124 returned 1000 rows.\n",
      "Batch 124 completed. Total rows saved: 124000\n",
      "Processing batch 125 (124000 to 125000 IDs)... Attempt 1\n",
      "Batch 125 returned 1000 rows.\n",
      "Batch 125 completed. Total rows saved: 125000\n",
      "Processing batch 126 (125000 to 126000 IDs)... Attempt 1\n",
      "Batch 126 returned 1000 rows.\n",
      "Batch 126 completed. Total rows saved: 126000\n",
      "Processing batch 127 (126000 to 127000 IDs)... Attempt 1\n",
      "Batch 127 returned 1000 rows.\n",
      "Batch 127 completed. Total rows saved: 127000\n",
      "Processing batch 128 (127000 to 128000 IDs)... Attempt 1\n",
      "Batch 128 returned 1000 rows.\n",
      "Batch 128 completed. Total rows saved: 128000\n",
      "Processing batch 129 (128000 to 129000 IDs)... Attempt 1\n",
      "Batch 129 returned 1000 rows.\n",
      "Batch 129 completed. Total rows saved: 129000\n",
      "Processing batch 130 (129000 to 130000 IDs)... Attempt 1\n",
      "Batch 130 returned 1000 rows.\n",
      "Batch 130 completed. Total rows saved: 130000\n",
      "Processing batch 131 (130000 to 131000 IDs)... Attempt 1\n",
      "Batch 131 returned 1000 rows.\n",
      "Batch 131 completed. Total rows saved: 131000\n",
      "Processing batch 132 (131000 to 132000 IDs)... Attempt 1\n",
      "Batch 132 returned 1000 rows.\n",
      "Batch 132 completed. Total rows saved: 132000\n",
      "Processing batch 133 (132000 to 133000 IDs)... Attempt 1\n",
      "Batch 133 returned 1000 rows.\n",
      "Batch 133 completed. Total rows saved: 133000\n",
      "Processing batch 134 (133000 to 134000 IDs)... Attempt 1\n",
      "Batch 134 returned 756 rows.\n",
      "Batch 134 completed. Total rows saved: 133756\n",
      "Reprocessing failed batches...\n",
      "All processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "batch_size = 1000  # Number of IDs per batch\n",
    "output_file = \"/home/thara/Big/gaia_phot_flux_results.csv\"  # Output file for results\n",
    "failed_batches_file = \"/home/thara/Big/failed_batches.txt\"  # Log file for failed batches\n",
    "max_retries = 3  # Number of retries for failed batches\n",
    "\n",
    "# Load the DataFrame containing SOURCE_ID\n",
    "source_ids = merged_results_df['SOURCE_ID'].dropna().unique()\n",
    "\n",
    "# Main batch processing loop\n",
    "for i in range(0, len(source_ids), batch_size):\n",
    "    # Create the batch query\n",
    "    batch_ids = \"','\".join(map(str, source_ids[i:i + batch_size]))\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT source_id, phot_bp_mean_flux, phot_rp_mean_flux\n",
    "    FROM gaiadr3.gaia_source\n",
    "    WHERE source_id IN ('{batch_ids}')\n",
    "    \"\"\"\n",
    "    \n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < max_retries and not success:\n",
    "        try:\n",
    "            print(f\"Processing batch {i // batch_size + 1} ({i} to {i + batch_size} IDs)... Attempt {retries + 1}\")\n",
    "            job = Gaia.launch_job(query)\n",
    "            batch_results = job.get_results().to_pandas()\n",
    "            print(f\"Batch {i // batch_size + 1} returned {len(batch_results)} rows.\")\n",
    "            \n",
    "            # Reload existing results\n",
    "            existing_results = pd.read_csv(output_file) if os.path.exists(output_file) else pd.DataFrame()\n",
    "            \n",
    "            # Combine results and remove duplicates\n",
    "            combined_results = pd.concat([existing_results, batch_results], ignore_index=True).drop_duplicates()\n",
    "            \n",
    "            # Save the updated results\n",
    "            combined_results.to_csv(output_file, index=False)\n",
    "            print(f\"Batch {i // batch_size + 1} completed. Total rows saved: {len(combined_results)}\")\n",
    "            \n",
    "            success = True  # Mark batch as successful\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error processing batch {i} to {i + batch_size}: {e}\")\n",
    "    \n",
    "    if not success:\n",
    "        # Log failed batch to file\n",
    "        with open(failed_batches_file, \"a\") as log_file:\n",
    "            log_file.write(f\"{i},{i + batch_size}\\n\")\n",
    "        print(f\"Batch {i} to {i + batch_size} failed after {max_retries} retries. Moving to the next batch.\")\n",
    "\n",
    "# Reprocess failed batches\n",
    "print(\"Reprocessing failed batches...\")\n",
    "try:\n",
    "    with open(failed_batches_file, \"r\") as log_file:\n",
    "        failed_batches = log_file.readlines()\n",
    "except FileNotFoundError:\n",
    "    failed_batches = []\n",
    "\n",
    "for batch_range in failed_batches:\n",
    "    start, end = map(int, batch_range.strip().split(\",\"))\n",
    "    batch_ids = \"','\".join(map(str, source_ids[start:end]))\n",
    "    query = f\"\"\"\n",
    "    SELECT source_id, phot_bp_mean_flux, phot_rp_mean_flux\n",
    "    FROM gaiadr3.gaia_source\n",
    "    WHERE source_id IN ('{batch_ids}')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Reprocessing batch {start} to {end}...\")\n",
    "        job = Gaia.launch_job(query)\n",
    "        batch_results = job.get_results().to_pandas()\n",
    "        \n",
    "        # Reload existing results\n",
    "        existing_results = pd.read_csv(output_file) if os.path.exists(output_file) else pd.DataFrame()\n",
    "        \n",
    "        # Combine results and remove duplicates\n",
    "        combined_results = pd.concat([existing_results, batch_results], ignore_index=True).drop_duplicates()\n",
    "        \n",
    "        # Save the updated results\n",
    "        combined_results.to_csv(output_file, index=False)\n",
    "        print(f\"Batch {start} to {end} reprocessed successfully. Total rows saved: {len(combined_results)}\")\n",
    "        \n",
    "        # Remove successfully reprocessed batch from log\n",
    "        with open(failed_batches_file, \"r\") as log_file:\n",
    "            lines = log_file.readlines()\n",
    "        with open(failed_batches_file, \"w\") as log_file:\n",
    "            log_file.writelines(line for line in lines if line.strip() != f\"{start},{end}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to reprocess batch {start} to {end}: {e}\")\n",
    "\n",
    "# Final message\n",
    "print(\"All processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f5e3d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE_ID</th>\n",
       "      <th>phot_bp_mean_flux</th>\n",
       "      <th>phot_rp_mean_flux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88751204298752</td>\n",
       "      <td>97.302780</td>\n",
       "      <td>80.055475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>190009353405440</td>\n",
       "      <td>119.288407</td>\n",
       "      <td>95.199147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5054111356737024</td>\n",
       "      <td>118.767233</td>\n",
       "      <td>183.231920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96986889894275200</td>\n",
       "      <td>57.062729</td>\n",
       "      <td>85.158148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100503746555005056</td>\n",
       "      <td>82.720489</td>\n",
       "      <td>72.286897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133751</th>\n",
       "      <td>5180145968212943744</td>\n",
       "      <td>563.933805</td>\n",
       "      <td>623.663854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133752</th>\n",
       "      <td>5186193389540051200</td>\n",
       "      <td>88.635141</td>\n",
       "      <td>93.043727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133753</th>\n",
       "      <td>5186205720391038336</td>\n",
       "      <td>78.457424</td>\n",
       "      <td>86.174115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133754</th>\n",
       "      <td>6898156118091837056</td>\n",
       "      <td>94.715035</td>\n",
       "      <td>125.106882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133755</th>\n",
       "      <td>6913378165089509760</td>\n",
       "      <td>94.442299</td>\n",
       "      <td>116.130014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133756 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  SOURCE_ID  phot_bp_mean_flux  phot_rp_mean_flux\n",
       "0            88751204298752          97.302780          80.055475\n",
       "1           190009353405440         119.288407          95.199147\n",
       "2          5054111356737024         118.767233         183.231920\n",
       "3         96986889894275200          57.062729          85.158148\n",
       "4        100503746555005056          82.720489          72.286897\n",
       "...                     ...                ...                ...\n",
       "133751  5180145968212943744         563.933805         623.663854\n",
       "133752  5186193389540051200          88.635141          93.043727\n",
       "133753  5186205720391038336          78.457424          86.174115\n",
       "133754  6898156118091837056          94.715035         125.106882\n",
       "133755  6913378165089509760          94.442299         116.130014\n",
       "\n",
       "[133756 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "775d3e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined DataFrame has 133756 rows.\n",
      "             bestObjID  plate    mjd  fiberid  flux_ratio  \\\n",
      "0  1237663784201486401   7868  57006      816    2.177181   \n",
      "1  1237665548902859021   2202  53566      290    2.016669   \n",
      "2  1237654380901957961   7512  56777      134    1.728434   \n",
      "3  1237655693011845153    918  52404      515    1.013675   \n",
      "4  1237667322185580645   2236  53729        1    0.747908   \n",
      "\n",
      "             SOURCE_ID  phot_bp_mean_flux  phot_rp_mean_flux  \n",
      "0  2543193557806316800         771.161658         812.454893  \n",
      "1  4463606678617356800         230.410150         216.745479  \n",
      "2  1014204471947028992          59.991009          88.311689  \n",
      "3  3652568217199181440         350.539988         705.208239  \n",
      "4  3961435288437544320         114.175541         260.146202  \n"
     ]
    }
   ],
   "source": [
    "# Merge the DataFrames on the SOURCE_ID column\n",
    "final_results_df = merged_results_df.merge(\n",
    "    combined_results,\n",
    "    on=\"SOURCE_ID\",  # Join on SOURCE_ID\n",
    "    how=\"inner\"  # Use inner join to keep only matching rows\n",
    ")\n",
    "\n",
    "# Print the number of rows and a preview\n",
    "print(f\"Joined DataFrame has {len(final_results_df)} rows.\")\n",
    "print(final_results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5278742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photometric flux ratio calculated and added to the DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bestObjID</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "      <th>flux_ratio</th>\n",
       "      <th>SOURCE_ID</th>\n",
       "      <th>phot_bp_mean_flux</th>\n",
       "      <th>phot_rp_mean_flux</th>\n",
       "      <th>photometric_flux_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1237663784201486401</td>\n",
       "      <td>7868</td>\n",
       "      <td>57006</td>\n",
       "      <td>816</td>\n",
       "      <td>2.177181</td>\n",
       "      <td>2543193557806316800</td>\n",
       "      <td>771.161658</td>\n",
       "      <td>812.454893</td>\n",
       "      <td>0.949175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1237665548902859021</td>\n",
       "      <td>2202</td>\n",
       "      <td>53566</td>\n",
       "      <td>290</td>\n",
       "      <td>2.016669</td>\n",
       "      <td>4463606678617356800</td>\n",
       "      <td>230.410150</td>\n",
       "      <td>216.745479</td>\n",
       "      <td>1.063045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237654380901957961</td>\n",
       "      <td>7512</td>\n",
       "      <td>56777</td>\n",
       "      <td>134</td>\n",
       "      <td>1.728434</td>\n",
       "      <td>1014204471947028992</td>\n",
       "      <td>59.991009</td>\n",
       "      <td>88.311689</td>\n",
       "      <td>0.679310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1237655693011845153</td>\n",
       "      <td>918</td>\n",
       "      <td>52404</td>\n",
       "      <td>515</td>\n",
       "      <td>1.013675</td>\n",
       "      <td>3652568217199181440</td>\n",
       "      <td>350.539988</td>\n",
       "      <td>705.208239</td>\n",
       "      <td>0.497073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1237667322185580645</td>\n",
       "      <td>2236</td>\n",
       "      <td>53729</td>\n",
       "      <td>1</td>\n",
       "      <td>0.747908</td>\n",
       "      <td>3961435288437544320</td>\n",
       "      <td>114.175541</td>\n",
       "      <td>260.146202</td>\n",
       "      <td>0.438890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133751</th>\n",
       "      <td>1237680273108960003</td>\n",
       "      <td>7572</td>\n",
       "      <td>56944</td>\n",
       "      <td>710</td>\n",
       "      <td>1.510391</td>\n",
       "      <td>1779912581508627072</td>\n",
       "      <td>98.424982</td>\n",
       "      <td>43.140794</td>\n",
       "      <td>2.281483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133752</th>\n",
       "      <td>1237680285996155390</td>\n",
       "      <td>6265</td>\n",
       "      <td>56248</td>\n",
       "      <td>512</td>\n",
       "      <td>1.797892</td>\n",
       "      <td>309194386402373120</td>\n",
       "      <td>48.999154</td>\n",
       "      <td>76.712520</td>\n",
       "      <td>0.638737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133753</th>\n",
       "      <td>1237680303715582376</td>\n",
       "      <td>6295</td>\n",
       "      <td>56536</td>\n",
       "      <td>434</td>\n",
       "      <td>0.798917</td>\n",
       "      <td>1883704073988238336</td>\n",
       "      <td>103.721589</td>\n",
       "      <td>334.565475</td>\n",
       "      <td>0.310019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133754</th>\n",
       "      <td>1237680308016841309</td>\n",
       "      <td>6515</td>\n",
       "      <td>56536</td>\n",
       "      <td>932</td>\n",
       "      <td>1.233320</td>\n",
       "      <td>2866659811294043904</td>\n",
       "      <td>164.762430</td>\n",
       "      <td>69.931613</td>\n",
       "      <td>2.356051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133755</th>\n",
       "      <td>1237680327333381385</td>\n",
       "      <td>5960</td>\n",
       "      <td>56097</td>\n",
       "      <td>436</td>\n",
       "      <td>1.524748</td>\n",
       "      <td>1799404173890542080</td>\n",
       "      <td>66.071696</td>\n",
       "      <td>78.717817</td>\n",
       "      <td>0.839349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133756 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bestObjID  plate    mjd  fiberid  flux_ratio  \\\n",
       "0       1237663784201486401   7868  57006      816    2.177181   \n",
       "1       1237665548902859021   2202  53566      290    2.016669   \n",
       "2       1237654380901957961   7512  56777      134    1.728434   \n",
       "3       1237655693011845153    918  52404      515    1.013675   \n",
       "4       1237667322185580645   2236  53729        1    0.747908   \n",
       "...                     ...    ...    ...      ...         ...   \n",
       "133751  1237680273108960003   7572  56944      710    1.510391   \n",
       "133752  1237680285996155390   6265  56248      512    1.797892   \n",
       "133753  1237680303715582376   6295  56536      434    0.798917   \n",
       "133754  1237680308016841309   6515  56536      932    1.233320   \n",
       "133755  1237680327333381385   5960  56097      436    1.524748   \n",
       "\n",
       "                  SOURCE_ID  phot_bp_mean_flux  phot_rp_mean_flux  \\\n",
       "0       2543193557806316800         771.161658         812.454893   \n",
       "1       4463606678617356800         230.410150         216.745479   \n",
       "2       1014204471947028992          59.991009          88.311689   \n",
       "3       3652568217199181440         350.539988         705.208239   \n",
       "4       3961435288437544320         114.175541         260.146202   \n",
       "...                     ...                ...                ...   \n",
       "133751  1779912581508627072          98.424982          43.140794   \n",
       "133752   309194386402373120          48.999154          76.712520   \n",
       "133753  1883704073988238336         103.721589         334.565475   \n",
       "133754  2866659811294043904         164.762430          69.931613   \n",
       "133755  1799404173890542080          66.071696          78.717817   \n",
       "\n",
       "        photometric_flux_ratio  \n",
       "0                     0.949175  \n",
       "1                     1.063045  \n",
       "2                     0.679310  \n",
       "3                     0.497073  \n",
       "4                     0.438890  \n",
       "...                        ...  \n",
       "133751                2.281483  \n",
       "133752                0.638737  \n",
       "133753                0.310019  \n",
       "133754                2.356051  \n",
       "133755                0.839349  \n",
       "\n",
       "[133756 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the required columns exist\n",
    "if 'phot_bp_mean_flux' in final_results_df.columns and 'phot_rp_mean_flux' in final_results_df.columns:\n",
    "    # Calculate the photometric flux ratio\n",
    "    final_results_df['photometric_flux_ratio'] = (\n",
    "        final_results_df['phot_bp_mean_flux'] / final_results_df['phot_rp_mean_flux']\n",
    "    )\n",
    "    \n",
    "    # Handle cases where grp_flux is zero to avoid division by zero errors\n",
    "    final_results_df['photometric_flux_ratio'] = final_results_df['photometric_flux_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    print(\"Photometric flux ratio calculated and added to the DataFrame.\")\n",
    "else:\n",
    "    print(\"Required columns 'phot_bp_mean_flux' and 'phot_rp_mean_flux' are missing.\")\n",
    "\n",
    "# Save the updated DataFrame\n",
    "final_results_df.to_csv(\"/home/thara/Big/final_results.csv\", index=False)\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "final_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3529bfa",
   "metadata": {},
   "source": [
    "Now we will proceed to the analysis of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
